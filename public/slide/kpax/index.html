<!DOCTYPE html>
<html>
  <head>
    <title>kpax</title>
    <meta charset='utf-8'>
    <script>
      var notesEnabled = false;
    </script>
    <script src='/static/slides.js'></script>
  </head>

  <body style='display: none'>

<section class='slides layout-widescreen'>

      <article>
        <h1>kpax</h1>
        <h3>a modular &amp; idiomatic Kafka client in Go</h3>
        <h3>21 August 2016</h3>
        
          <div class="presenter">
            
  
  <p>
    Hǎi-Liàng &#34;Hal&#34; Wáng
  </p>
  

  
  <p>
    Gopher at Appcoach
  </p>
  

          </div>
        
      </article>

  
  
      <article>
      
        <h3>Introduction</h3>
        
  
  <p>
    About Me
  </p>
  

  <ul>
  
    <li>Pure Gopher since 2012</li>
  
    <li>Backend Lead at Appcoach</li>
  
    <li>https://h12.io</li>
  
  </ul>

  
  <p>
    About Appcoach
  </p>
  

  <ul>
  
    <li>Global mobile advertising agency founded in 2014</li>
  
    <li>All backend services are written in Go</li>
  
    <li>http://www.appcoachs.com</li>
  
  </ul>

      
      </article>
  
  
  
      <article>
      
        <h2>Rationale</h2>
      
      </article>
  
  
  
      <article>
      
        <h3>Why Kafka?</h3>
        
  
  <p>
    No resources to build our own messaging system from scratch :-)
  </p>
  

  
  <p>
    Kafka has excellent design and robust implementation:
  </p>
  

  <ul>
  
    <li>A distributed, persisted, replicated messaging system</li>
  
    <li>Key concepts: topic, partition, offset, retention, replication-factor, partition leader, consumer group, broker/cluster</li>
  
  </ul>

<div class="image">
  <img src="/img/kpax/kafka.png">
</div>

      
      </article>
  
  
  
      <article>
      
        <h3>Why yet another Go client for Kafka?</h3>
        
  
  <p>
    Disappointed about the unnecessary complexity of
  </p>
  

  <ul>
  
    <li><code>sarama</code></li>
  
    <li><code>siesta</code> &amp; <code>go_kafka_client</code></li>
  
  </ul>

  
  <p>
    Gain better understanding about Kafka by writing a client and know how to troubleshoot when problems occur
  </p>
  

      
      </article>
  
  
  
      <article>
      
        <h3>Goal</h3>
        
  <ul>
  
    <li>Simplicity: a thin layer above Kafka Wire Protocol, avoiding abuse of goroutines and channels</li>
  
    <li>Modularity: separation of mechanism and policy, allowing the policy code to be easily replaced</li>
  
    <li>Verifiability: unit test, manual checking with CLI and online data tracing</li>
  
  </ul>

      
      </article>
  
  
  
      <article>
      
        <h2>Kafka Wire Protocol</h2>
      
      </article>
  
  
  
      <article>
      
        <h3>BNF</h3>
        
  
  <p>
    From http://kafka.apache.org/protocol.html:
  </p>
  

  
  <div class="code"><pre>MessageSet =&gt; &lt;OffsetMessage&gt;
    OffsetMessage =&gt; Offset SizedMessage
    Offset =&gt; int64
    SizedMessage =&gt; Size CrcMessage
    Size =&gt; int32
    CrcMessage =&gt; Crc Message
    Crc =&gt; uint32

Message =&gt; MagicByte Attributes Key Value
    MagicByte =&gt; int8
    Attributes =&gt; int8
    Key =&gt; bytes
    Value =&gt; bytes</pre></div>
  

      
      </article>
  
  
  
      <article>
      
        <h3>Code Generation</h3>
        
  
  <p>
    By h12.io/wipro:
  </p>
  

  
  <div class="code"><pre>func (t *MessageSet) Marshal(w *wipro.Writer) {
    offset := len(w.B)
    w.WriteInt32(0)
    start := len(w.B)
    for i := range *t {
        (*t)[i].Marshal(w)
    }
    w.SetInt32(offset, int32(len(w.B)-start))
}

func (t *Message) Marshal(w *wipro.Writer) {
    w.WriteInt8(t.MagicByte)
    w.WriteInt8(t.Attributes)
    w.WriteBytes(t.Key)
    w.WriteBytes(t.Value)
}</pre></div>
  

      
      </article>
  
  
  
      <article>
      
        <h3>Broker</h3>
        
  
  <p>
    Kafka Wire Protocol:
  </p>
  

  
  <p>
    &#34;It should not generally be necessary to maintain multiple connections to a single broker from a single client instance (i.e. connection pooling)&#34;
  </p>
  

  
  <p>
    &#34;Clients can (and ideally should) use non-blocking IO to implement request pipelining and achieve higher throughput&#34;
  </p>
  

<div class="image">
  <img src="/img/kpax/async-io.png" height="250">
</div>

      
      </article>
  
  
  
      <article>
      
        <h2>Wrap It Up</h2>
      
      </article>
  
  
  
      <article>
      
        <h3>Code Organization</h3>
        
  
  <p>
    Modular and reusable sub-packages:
  </p>
  

  
  <div class="code"><pre>h12.io/kpax/
    proto
    broker
    cluster
    producer
    consumer
    log</pre></div>
  

      
      </article>
  
  
  
      <article>
      
        <h3>A Simple &amp; Naive Approach</h3>
        
  
  <p>
    Broker &gt; Cluster &gt; Producer
  </p>
  

  
  <p>
    Produce Methods:
  </p>
  

  
  <div class="code"><pre>func (b *Broker) Produce(.......)
func (b *Cluster) Produce(.......)
func (b *Producer) Produce(.......)</pre></div>
  

  
  <p>
    Constructers:
  </p>
  

  
  <div class="code"><pre>broker.New(cfg *BrokerConfig)
cluster.New(cfg *ClusterConfig)
producer.New(cfg *ProducerConfig)</pre></div>
  

  
  <p>
    Problems of this approach
  </p>
  

  <ul>
  
    <li>Duplicated code</li>
  
    <li>Low and high level are tightly coupled</li>
  
    <li>Policies in lower level components cannot be replaced</li>
  
  </ul>

      
      </article>
  
  
  
      <article>
      
        <h3>Dependency Inversion Principle</h3>
        
  
  <p>
    DIP:
  </p>
  

  <ul>
  
    <li>High-level modules should not depend on low-level modules. Both should depend on abstractions.</li>
  
    <li>Abstractions should not depend on details. Details should depend on abstractions.</li>
  
  </ul>

  
  <p>
    In our case:
  </p>
  

  <ul>
  
    <li>Broker/Cluster level should not depend on protocol-level details</li>
  
    <li>Protocol-level details should depend on abstract broker/cluster</li>
  
  </ul>

      
      </article>
  
  
  
      <article>
      
        <h3>Abstraction</h3>
        
  
  <div class="code"><pre>type Broker interface {
    Do(Request, Response) error
    Close()
}

type Cluster interface {
    Coordinator(group string) (Broker, error)
    CoordinatorIsDown(group string)
    Leader(topic string, partition int32) (Broker, error)
    LeaderIsDown(topic string, partition int32)
    Partitions(topic string) ([]int32, error)
}

type Request interface {
    Send(io.Writer) error
    ID() int32
    SetID(int32)
}

type Response interface {
    Receive(io.Reader) error
    ID() int32
}</pre></div>
  

      
      </article>
  
  
  
      <article>
      
        <h3>Details</h3>
        
  
  <p>
    Payload.Produce
  </p>
  

  
  <div class="code"><pre>func (p *Payload) Produce(c model.Cluster) error {
    leader, err := c.Leader(p.Topic, p.Partition)
    if err != nil {
        return err
    }
    if err := p.DoProduce(leader); err != nil {
        if IsNotLeader(err) {
            c.LeaderIsDown(p.Topic, p.Partition)
        }
        return err
    }
    return nil
}

func (p *Payload) DoProduce(b model.Broker) error {
    ......
    if err := b.Do(&amp;req, &amp;resp); err != nil {
        return err
    }
    ......
}</pre></div>
  

      
      </article>
  
  
  
      <article>
      
        <h3>Configuration</h3>
        
  
  <div class="code"><pre>type Payload struct {
    Topic        string
    Partition    int32
    MessageSet   MessageSet
    RequiredAcks ProduceAckType
    AckTimeout   time.Duration
}

type Broker struct {
    Addr     string
    Timeout  time.Duration
}

type Producer struct {
    Cluster      model.Cluster
    RequiredAcks proto.ProduceAckType
    AckTimeout   time.Duration
}

NewCluster(newBroker NewBrokerFunc, brokers []string) model.Cluster</pre></div>
  

  
  <p>
    &#34;New&#34; constructor returns an object with default configuration.
  </p>
  

      
      </article>
  
  
  
      <article>
      
        <h3>Command Line Tool</h3>
        
  
  <div class="code"><pre>go install h12.io/kpax/cmd/kpax</pre></div>
  

  
  <p>
    Sub-commands:
  </p>
  

  <ul>
  
    <li>tail: print recent n messages</li>
  
    <li>consume: print or count messages within a time range</li>
  
    <li>produce: send a test message</li>
  
    <li>offset: print latest/earliest/current offset</li>
  
    <li>rollback: reset offset to a previous time</li>
  
    <li>meta: print topic metadata</li>
  
  </ul>

      
      </article>
  
  
  
      <article>
      
        <h2>Verification</h2>
      
      </article>
  
  
  
      <article>
      
        <h3>Unit Testing with Docker</h3>
        
  
  <p>
    Using h12.io/realtest/kafka:
  </p>
  

  
  <div class="code"><pre>func TestXXX(t *testing.T) {
    k, err := kafka.New()
    if err != nil {
        t.Fatal(err)
    }
    partitionCount := 2
    topic, err := k.NewRandomTopic(partitionCount)
    if err != nil {
        t.Fatal(err)
    }
    defer k.DeleteTopic(topic)

    // Your test code goes here
}</pre></div>
  

      
      </article>
  
  
  
      <article>
      
        <h3>Message Counting</h3>
        
  
  <p>
    Manual checking:
  </p>
  

  <ul>
  
    <li>use kpax command line tool</li>
  
  </ul>

  
  <p>
    Continuous monitoring:
  </p>
  

  <ul>
  
    <li>Consume and count all messages</li>
  
    <li>Collect statistics from API</li>
  
    <li>Send all the statistics to InfluxDB</li>
  
    <li>Visualize and compare with Grafana</li>
  
  </ul>

      
      </article>
  
  
  
      <article>
      
        <h2>Some Thoughts on Messaging System</h2>
      
      </article>
  
  
  
      <article>
      
        <h3>When a Messaging System could be Overkill?</h3>
        
  <ul>
  
    <li>Data is disposable</li>
  
    <li>Asynchronous: goroutine</li>
  
    <li>Batch buffer: buffered channel</li>
  
    <li>Decoupling: backend API</li>
  
    <li>Resiliency/Scalability: distributed backend API</li>
  
  </ul>

      
      </article>
  
  
  
      <article>
      
        <h3>When a Messaging System INDEED useful?</h3>
        
  <ul>
  
    <li>Delivery guarantees: provide &#34;commit&#34; semantics to the writer (i.e. acknowledging only when your write guaranteed not to be lost)</li>
  
    <li>Order guarantees: handle data consistency by sequencing concurrent updates to nodes</li>
  
    <li>Multiple subscribers: provide the external data subscription feed from the system</li>
  
  </ul>

      
      </article>
  
  
  
      <article>
      
        <h3>How to achieve reliable persistence?</h3>
        
  
  <p>
    Crash consistency is hard but possible at a huge performance cost (fsync)
  </p>
  

  
  <p>
    To get short response time, perhaps we should either give up persistence,
  </p>
  

  
  <p>
    or we could just give up manual fsync and rely on replication:
  </p>
  

  <ul>
  
    <li>OS background flush</li>
  
    <li>replication</li>
  
    <li>corruption detection &amp; correction at startup</li>
  
  </ul>

      
      </article>
  
  

      <article>
        <h3>Thank you</h3>
        
          <div class="presenter">
            
  
  <p>
    Hǎi-Liàng &#34;Hal&#34; Wáng
  </p>
  

  
  <p>
    Gopher at Appcoach
  </p>
  
<p class="link"><a href="mailto:hal@h12.io" target="_blank">hal@h12.io</a></p>
          </div>
        
      </article>

    </section>

    



    <div id="help">
      Use the left and right arrow keys or click the left and right
      edges of the page to navigate between slides.<br>
      (Press 'H' or navigate to hide this message.)
    </div>

    <script src='/static/play.js'></script>

  </body>
</html>
