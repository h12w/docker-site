<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Slides on h12.me</title>
    <link>http://h12.me/slide/index.xml</link>
    <description>Recent content in Slides on h12.me</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Copyright (c) 2012-2018, Hǎiliàng Wáng; all rights reserved.</copyright>
    <lastBuildDate>Sun, 21 Aug 2016 13:00:00 +0000</lastBuildDate>
    <atom:link href="http://h12.me/slide/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>kpax</title>
      <link>http://h12.me/slide/kpax/</link>
      <pubDate>Sun, 21 Aug 2016 13:00:00 +0000</pubDate>
      
      <guid>http://h12.me/slide/kpax/</guid>
      <description>&lt;section class=&#39;slides layout-widescreen&#39;&gt;

      &lt;article&gt;
        &lt;h1&gt;kpax&lt;/h1&gt;
        &lt;h3&gt;a modular &amp;amp; idiomatic Kafka client in Go&lt;/h3&gt;
        &lt;h3&gt;21 August 2016&lt;/h3&gt;
        
          &lt;div class=&#34;presenter&#34;&gt;
            
  
  &lt;p&gt;
    Wáng Hǎiliàng
  &lt;/p&gt;
  

  
  &lt;p&gt;
    Gopher at Appcoach
  &lt;/p&gt;
  

          &lt;/div&gt;
        
      &lt;/article&gt;

  
  
      &lt;article&gt;
      
        &lt;h3&gt;Introduction&lt;/h3&gt;
        
  
  &lt;p&gt;
    About Me
  &lt;/p&gt;
  

  &lt;ul&gt;
  
    &lt;li&gt;Pure Gopher since 2012&lt;/li&gt;
  
    &lt;li&gt;Backend Lead at Appcoach&lt;/li&gt;
  
    &lt;li&gt;https://h12.me&lt;/li&gt;
  
  &lt;/ul&gt;

  
  &lt;p&gt;
    About Appcoach
  &lt;/p&gt;
  

  &lt;ul&gt;
  
    &lt;li&gt;Global mobile advertising agency founded in 2014&lt;/li&gt;
  
    &lt;li&gt;All backend services are written in Go&lt;/li&gt;
  
    &lt;li&gt;http://www.appcoachs.com&lt;/li&gt;
  
  &lt;/ul&gt;

      
      &lt;/article&gt;
  
  
  
      &lt;article&gt;
      
        &lt;h2&gt;Rationale&lt;/h2&gt;
      
      &lt;/article&gt;
  
  
  
      &lt;article&gt;
      
        &lt;h3&gt;Why Kafka?&lt;/h3&gt;
        
  
  &lt;p&gt;
    No resources to build our own messaging system from scratch :-)
  &lt;/p&gt;
  

  
  &lt;p&gt;
    Kafka has excellent design and robust implementation:
  &lt;/p&gt;
  

  &lt;ul&gt;
  
    &lt;li&gt;A distributed, persisted, replicated messaging system&lt;/li&gt;
  
    &lt;li&gt;Key concepts: topic, partition, offset, retention, replication-factor, partition leader, consumer group, broker/cluster&lt;/li&gt;
  
  &lt;/ul&gt;

&lt;div class=&#34;image&#34;&gt;
  &lt;img src=&#34;http://h12.me/img/kpax/kafka.png&#34;&gt;
&lt;/div&gt;

      
      &lt;/article&gt;
  
  
  
      &lt;article&gt;
      
        &lt;h3&gt;Why yet another Go client for Kafka?&lt;/h3&gt;
        
  
  &lt;p&gt;
    Disappointed about the unnecessary complexity of
  &lt;/p&gt;
  

  &lt;ul&gt;
  
    &lt;li&gt;&lt;code&gt;sarama&lt;/code&gt;&lt;/li&gt;
  
    &lt;li&gt;&lt;code&gt;siesta&lt;/code&gt; &amp;amp; &lt;code&gt;go_kafka_client&lt;/code&gt;&lt;/li&gt;
  
  &lt;/ul&gt;

  
  &lt;p&gt;
    Gain better understanding about Kafka by writing a client and know how to troubleshoot when problems occur
  &lt;/p&gt;
  

      
      &lt;/article&gt;
  
  
  
      &lt;article&gt;
      
        &lt;h3&gt;Goal&lt;/h3&gt;
        
  &lt;ul&gt;
  
    &lt;li&gt;Simplicity: a thin layer above Kafka Wire Protocol, avoiding abuse of goroutines and channels&lt;/li&gt;
  
    &lt;li&gt;Modularity: separation of mechanism and policy, allowing the policy code to be easily replaced&lt;/li&gt;
  
    &lt;li&gt;Verifiability: unit test, manual checking with CLI and online data tracing&lt;/li&gt;
  
  &lt;/ul&gt;

      
      &lt;/article&gt;
  
  
  
      &lt;article&gt;
      
        &lt;h2&gt;Kafka Wire Protocol&lt;/h2&gt;
      
      &lt;/article&gt;
  
  
  
      &lt;article&gt;
      
        &lt;h3&gt;BNF&lt;/h3&gt;
        
  
  &lt;p&gt;
    From http://kafka.apache.org/protocol.html:
  &lt;/p&gt;
  

  
  &lt;div class=&#34;code&#34;&gt;&lt;pre&gt;MessageSet =&amp;gt; &amp;lt;OffsetMessage&amp;gt;
    OffsetMessage =&amp;gt; Offset SizedMessage
    Offset =&amp;gt; int64
    SizedMessage =&amp;gt; Size CrcMessage
    Size =&amp;gt; int32
    CrcMessage =&amp;gt; Crc Message
    Crc =&amp;gt; uint32

Message =&amp;gt; MagicByte Attributes Key Value
    MagicByte =&amp;gt; int8
    Attributes =&amp;gt; int8
    Key =&amp;gt; bytes
    Value =&amp;gt; bytes&lt;/pre&gt;&lt;/div&gt;
  

      
      &lt;/article&gt;
  
  
  
      &lt;article&gt;
      
        &lt;h3&gt;Code Generation&lt;/h3&gt;
        
  
  &lt;p&gt;
    By h12.me/wipro:
  &lt;/p&gt;
  

  
  &lt;div class=&#34;code&#34;&gt;&lt;pre&gt;func (t *MessageSet) Marshal(w *wipro.Writer) {
    offset := len(w.B)
    w.WriteInt32(0)
    start := len(w.B)
    for i := range *t {
        (*t)[i].Marshal(w)
    }
    w.SetInt32(offset, int32(len(w.B)-start))
}

func (t *Message) Marshal(w *wipro.Writer) {
    w.WriteInt8(t.MagicByte)
    w.WriteInt8(t.Attributes)
    w.WriteBytes(t.Key)
    w.WriteBytes(t.Value)
}&lt;/pre&gt;&lt;/div&gt;
  

      
      &lt;/article&gt;
  
  
  
      &lt;article&gt;
      
        &lt;h3&gt;Broker&lt;/h3&gt;
        
  
  &lt;p&gt;
    Kafka Wire Protocol:
  &lt;/p&gt;
  

  
  &lt;p&gt;
    &amp;#34;It should not generally be necessary to maintain multiple connections to a single broker from a single client instance (i.e. connection pooling)&amp;#34;
  &lt;/p&gt;
  

  
  &lt;p&gt;
    &amp;#34;Clients can (and ideally should) use non-blocking IO to implement request pipelining and achieve higher throughput&amp;#34;
  &lt;/p&gt;
  

&lt;div class=&#34;image&#34;&gt;
  &lt;img src=&#34;http://h12.me/img/kpax/async-io.png&#34; height=&#34;250&#34;&gt;
&lt;/div&gt;

      
      &lt;/article&gt;
  
  
  
      &lt;article&gt;
      
        &lt;h2&gt;Wrap It Up&lt;/h2&gt;
      
      &lt;/article&gt;
  
  
  
      &lt;article&gt;
      
        &lt;h3&gt;Code Organization&lt;/h3&gt;
        
  
  &lt;p&gt;
    Modular and reusable sub-packages:
  &lt;/p&gt;
  

  
  &lt;div class=&#34;code&#34;&gt;&lt;pre&gt;h12.me/kpax/
    proto
    broker
    cluster
    producer
    consumer
    log&lt;/pre&gt;&lt;/div&gt;
  

      
      &lt;/article&gt;
  
  
  
      &lt;article&gt;
      
        &lt;h3&gt;A Simple &amp;amp; Naive Approach&lt;/h3&gt;
        
  
  &lt;p&gt;
    Broker &amp;gt; Cluster &amp;gt; Producer
  &lt;/p&gt;
  

  
  &lt;p&gt;
    Produce Methods:
  &lt;/p&gt;
  

  
  &lt;div class=&#34;code&#34;&gt;&lt;pre&gt;func (b *Broker) Produce(.......)
func (b *Cluster) Produce(.......)
func (b *Producer) Produce(.......)&lt;/pre&gt;&lt;/div&gt;
  

  
  &lt;p&gt;
    Constructers:
  &lt;/p&gt;
  

  
  &lt;div class=&#34;code&#34;&gt;&lt;pre&gt;broker.New(cfg *BrokerConfig)
cluster.New(cfg *ClusterConfig)
producer.New(cfg *ProducerConfig)&lt;/pre&gt;&lt;/div&gt;
  

  
  &lt;p&gt;
    Problems of this approach
  &lt;/p&gt;
  

  &lt;ul&gt;
  
    &lt;li&gt;Duplicated code&lt;/li&gt;
  
    &lt;li&gt;Low and high level are tightly coupled&lt;/li&gt;
  
    &lt;li&gt;Policies in lower level components cannot be replaced&lt;/li&gt;
  
  &lt;/ul&gt;

      
      &lt;/article&gt;
  
  
  
      &lt;article&gt;
      
        &lt;h3&gt;Dependency Inversion Principle&lt;/h3&gt;
        
  
  &lt;p&gt;
    DIP:
  &lt;/p&gt;
  

  &lt;ul&gt;
  
    &lt;li&gt;High-level modules should not depend on low-level modules. Both should depend on abstractions.&lt;/li&gt;
  
    &lt;li&gt;Abstractions should not depend on details. Details should depend on abstractions.&lt;/li&gt;
  
  &lt;/ul&gt;

  
  &lt;p&gt;
    In our case:
  &lt;/p&gt;
  

  &lt;ul&gt;
  
    &lt;li&gt;Broker/Cluster level should not depend on protocol-level details&lt;/li&gt;
  
    &lt;li&gt;Protocol-level details should depend on abstract broker/cluster&lt;/li&gt;
  
  &lt;/ul&gt;

      
      &lt;/article&gt;
  
  
  
      &lt;article&gt;
      
        &lt;h3&gt;Abstraction&lt;/h3&gt;
        
  
  &lt;div class=&#34;code&#34;&gt;&lt;pre&gt;type Broker interface {
    Do(Request, Response) error
    Close()
}

type Cluster interface {
    Coordinator(group string) (Broker, error)
    CoordinatorIsDown(group string)
    Leader(topic string, partition int32) (Broker, error)
    LeaderIsDown(topic string, partition int32)
    Partitions(topic string) ([]int32, error)
}

type Request interface {
    Send(io.Writer) error
    ID() int32
    SetID(int32)
}

type Response interface {
    Receive(io.Reader) error
    ID() int32
}&lt;/pre&gt;&lt;/div&gt;
  

      
      &lt;/article&gt;
  
  
  
      &lt;article&gt;
      
        &lt;h3&gt;Details&lt;/h3&gt;
        
  
  &lt;p&gt;
    Payload.Produce
  &lt;/p&gt;
  

  
  &lt;div class=&#34;code&#34;&gt;&lt;pre&gt;func (p *Payload) Produce(c model.Cluster) error {
    leader, err := c.Leader(p.Topic, p.Partition)
    if err != nil {
        return err
    }
    if err := p.DoProduce(leader); err != nil {
        if IsNotLeader(err) {
            c.LeaderIsDown(p.Topic, p.Partition)
        }
        return err
    }
    return nil
}

func (p *Payload) DoProduce(b model.Broker) error {
    ......
    if err := b.Do(&amp;amp;req, &amp;amp;resp); err != nil {
        return err
    }
    ......
}&lt;/pre&gt;&lt;/div&gt;
  

      
      &lt;/article&gt;
  
  
  
      &lt;article&gt;
      
        &lt;h3&gt;Configuration&lt;/h3&gt;
        
  
  &lt;div class=&#34;code&#34;&gt;&lt;pre&gt;type Payload struct {
    Topic        string
    Partition    int32
    MessageSet   MessageSet
    RequiredAcks ProduceAckType
    AckTimeout   time.Duration
}

type Broker struct {
    Addr     string
    Timeout  time.Duration
}

type Producer struct {
    Cluster      model.Cluster
    RequiredAcks proto.ProduceAckType
    AckTimeout   time.Duration
}

NewCluster(newBroker NewBrokerFunc, brokers []string) model.Cluster&lt;/pre&gt;&lt;/div&gt;
  

  
  &lt;p&gt;
    &amp;#34;New&amp;#34; constructor returns an object with default configuration.
  &lt;/p&gt;
  

      
      &lt;/article&gt;
  
  
  
      &lt;article&gt;
      
        &lt;h3&gt;Command Line Tool&lt;/h3&gt;
        
  
  &lt;div class=&#34;code&#34;&gt;&lt;pre&gt;go install h12.me/kpax/cmd/kpax&lt;/pre&gt;&lt;/div&gt;
  

  
  &lt;p&gt;
    Sub-commands:
  &lt;/p&gt;
  

  &lt;ul&gt;
  
    &lt;li&gt;tail: print recent n messages&lt;/li&gt;
  
    &lt;li&gt;consume: print or count messages within a time range&lt;/li&gt;
  
    &lt;li&gt;produce: send a test message&lt;/li&gt;
  
    &lt;li&gt;offset: print latest/earliest/current offset&lt;/li&gt;
  
    &lt;li&gt;rollback: reset offset to a previous time&lt;/li&gt;
  
    &lt;li&gt;meta: print topic metadata&lt;/li&gt;
  
  &lt;/ul&gt;

      
      &lt;/article&gt;
  
  
  
      &lt;article&gt;
      
        &lt;h2&gt;Verification&lt;/h2&gt;
      
      &lt;/article&gt;
  
  
  
      &lt;article&gt;
      
        &lt;h3&gt;Unit Testing with Docker&lt;/h3&gt;
        
  
  &lt;p&gt;
    Using h12.me/realtest/kafka:
  &lt;/p&gt;
  

  
  &lt;div class=&#34;code&#34;&gt;&lt;pre&gt;func TestXXX(t *testing.T) {
    k, err := kafka.New()
    if err != nil {
        t.Fatal(err)
    }
    partitionCount := 2
    topic, err := k.NewRandomTopic(partitionCount)
    if err != nil {
        t.Fatal(err)
    }
    defer k.DeleteTopic(topic)

    // Your test code goes here
}&lt;/pre&gt;&lt;/div&gt;
  

      
      &lt;/article&gt;
  
  
  
      &lt;article&gt;
      
        &lt;h3&gt;Message Counting&lt;/h3&gt;
        
  
  &lt;p&gt;
    Manual checking:
  &lt;/p&gt;
  

  &lt;ul&gt;
  
    &lt;li&gt;use kpax command line tool&lt;/li&gt;
  
  &lt;/ul&gt;

  
  &lt;p&gt;
    Continuous monitoring:
  &lt;/p&gt;
  

  &lt;ul&gt;
  
    &lt;li&gt;Consume and count all messages&lt;/li&gt;
  
    &lt;li&gt;Collect statistics from API&lt;/li&gt;
  
    &lt;li&gt;Send all the statistics to InfluxDB&lt;/li&gt;
  
    &lt;li&gt;Visualize and compare with Grafana&lt;/li&gt;
  
  &lt;/ul&gt;

      
      &lt;/article&gt;
  
  
  
      &lt;article&gt;
      
        &lt;h2&gt;Some Thoughts on Messaging System&lt;/h2&gt;
      
      &lt;/article&gt;
  
  
  
      &lt;article&gt;
      
        &lt;h3&gt;When a Messaging System could be Overkill?&lt;/h3&gt;
        
  &lt;ul&gt;
  
    &lt;li&gt;Data is disposable&lt;/li&gt;
  
    &lt;li&gt;Asynchronous: goroutine&lt;/li&gt;
  
    &lt;li&gt;Batch buffer: buffered channel&lt;/li&gt;
  
    &lt;li&gt;Decoupling: backend API&lt;/li&gt;
  
    &lt;li&gt;Resiliency/Scalability: distributed backend API&lt;/li&gt;
  
  &lt;/ul&gt;

      
      &lt;/article&gt;
  
  
  
      &lt;article&gt;
      
        &lt;h3&gt;When a Messaging System INDEED useful?&lt;/h3&gt;
        
  &lt;ul&gt;
  
    &lt;li&gt;Delivery guarantees: provide &amp;#34;commit&amp;#34; semantics to the writer (i.e. acknowledging only when your write guaranteed not to be lost)&lt;/li&gt;
  
    &lt;li&gt;Order guarantees: handle data consistency by sequencing concurrent updates to nodes&lt;/li&gt;
  
    &lt;li&gt;Multiple subscribers: provide the external data subscription feed from the system&lt;/li&gt;
  
  &lt;/ul&gt;

      
      &lt;/article&gt;
  
  
  
      &lt;article&gt;
      
        &lt;h3&gt;How to achieve reliable persistence?&lt;/h3&gt;
        
  
  &lt;p&gt;
    Crash consistency is hard but possible at a huge performance cost (fsync)
  &lt;/p&gt;
  

  
  &lt;p&gt;
    To get short response time, perhaps we should either give up persistence,
  &lt;/p&gt;
  

  
  &lt;p&gt;
    or we could just give up manual fsync and rely on replication:
  &lt;/p&gt;
  

  &lt;ul&gt;
  
    &lt;li&gt;OS background flush&lt;/li&gt;
  
    &lt;li&gt;replication&lt;/li&gt;
  
    &lt;li&gt;corruption detection &amp;amp; correction at startup&lt;/li&gt;
  
  &lt;/ul&gt;

      
      &lt;/article&gt;
  
  

      &lt;article&gt;
        &lt;h3&gt;Thank you&lt;/h3&gt;
        
          &lt;div class=&#34;presenter&#34;&gt;
            
  
  &lt;p&gt;
    Wáng Hǎiliàng
  &lt;/p&gt;
  

  
  &lt;p&gt;
    Gopher at Appcoach
  &lt;/p&gt;
  
&lt;p class=&#34;link&#34;&gt;&lt;a href=&#34;mailto:w@h12.me&#34; target=&#34;_blank&#34;&gt;w@h12.me&lt;/a&gt;&lt;/p&gt;
          &lt;/div&gt;
        
      &lt;/article&gt;

    &lt;/section&gt;

    

</description>
    </item>
    
  </channel>
</rss>